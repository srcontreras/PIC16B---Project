{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b9041b4b-4288-42d5-a6db-67127b6051e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/sarah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "# for model \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from textblob import Word\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import StringLookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "556c7a2e-060a-4f0f-aee2-112092c23dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Butler University Creates 2-Year Debt-Free Col...</td>\n",
       "      <td>Butler University of Indianapolis has created ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since Pandemic Closed His Business, New Jersey...</td>\n",
       "      <td>The owner of a New Jersey frame shop has been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PayPal Commits Over $500 Million to Support Mi...</td>\n",
       "      <td>PayPal yesterday announced a $530 million comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9-Year-Old and Friends Have Raised $100,000 fo...</td>\n",
       "      <td>Some unlikely heroes in Minneapolis have raise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hanes is Equipping America’s Homeless With 1 M...</td>\n",
       "      <td>Hanes basic apparel is not only encouraging Am...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Butler University Creates 2-Year Debt-Free Col...   \n",
       "1  Since Pandemic Closed His Business, New Jersey...   \n",
       "2  PayPal Commits Over $500 Million to Support Mi...   \n",
       "3  9-Year-Old and Friends Have Raised $100,000 fo...   \n",
       "4  Hanes is Equipping America’s Homeless With 1 M...   \n",
       "\n",
       "                                             content  \n",
       "0  Butler University of Indianapolis has created ...  \n",
       "1  The owner of a New Jersey frame shop has been ...  \n",
       "2  PayPal yesterday announced a $530 million comm...  \n",
       "3  Some unlikely heroes in Minneapolis have raise...  \n",
       "4  Hanes basic apparel is not only encouraging Am...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in good news data\n",
    "df = pd.read_csv(\"articles.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756e5b1-f64a-4e8f-8d15-5063bb61e5e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **MODEL 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c599aada-c8a0-47de-98ac-5d432076d583",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "2f886ff8-a7d7-4cfd-baf2-b8920b0d918e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Sentiment\n",
       "0  The GeoSolutions technology will leverage Bene...  positive\n",
       "1  $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n",
       "2  For the last quarter of 2010 , Componenta 's n...  positive\n",
       "3  According to the Finnish-Russian Chamber of Co...   neutral\n",
       "4  The Swedish buyout firm has sold its remaining...   neutral"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load financial sentiment analysis data\n",
    "# https://www.kaggle.com/datasets/sbhatti/financial-sentiment-analysis\n",
    "# data provides a column for financial sentences \n",
    "# and a column for sentiment ('positive', 'negative', or 'neutral')\n",
    "fin_data = pd.read_csv(\"fin_data.csv\")\n",
    "fin_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda85cac-d329-4416-ab20-fac8db7106c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca43c6a6-47e8-426d-87c2-e11771184725",
   "metadata": {},
   "source": [
    "### Label Encoder\n",
    "We want to transform the Sentiment column into integers using a label encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e40a326f-333e-4176-8494-2ee4eb109b99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Sentiment\n",
       "0  The GeoSolutions technology will leverage Bene...         2\n",
       "1  $ESI on lows, down $1.50 to $2.50 BK a real po...         0\n",
       "2  For the last quarter of 2010 , Componenta 's n...         2\n",
       "3  According to the Finnish-Russian Chamber of Co...         1\n",
       "4  The Swedish buyout firm has sold its remaining...         1"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "fin_data.loc[:, \"Sentiment\"] = le.fit_transform(fin_data[\"Sentiment\"])\n",
    "fin_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "8a8bfe27-001f-4b75-b889-c59ca9f832bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'neutral', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_ # 0 = negative, 1 = neautral, 2 = positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e749b6-c6f5-4c4a-a64c-9b52456c3ac9",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "We create a `standardization()` function to clean the text for our data later on. The `standardization()` function outputs text in lower case, removes punctuation, digits, and stop words, and lemmatizes the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "bf0d5dbc-a42c-4452-b5c9-fdf1e48b53eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def standardization(df, col_name):\n",
    "    stop_words = stopwords.words('english')  \n",
    "    # transforms text to lowercase letters\n",
    "    df[col_name] = df[col_name].str.lower()\n",
    "    # remove digits \n",
    "    df[col_name] = df[col_name].apply(lambda x: ''.join(d for d in x if not d.isdigit()))\n",
    "    # remove punctuation\n",
    "    df[col_name] = df[col_name].apply(lambda x: ''.join(x for x in x if x not in string.punctuation))\n",
    "    # removes stop words for each word in Sentence column\n",
    "    df[col_name] = df[col_name].apply(lambda x: ' '.join(x for x in x.split() if x not in stop_words))\n",
    "    # lemmatization: reduce inflected words to root form for each word in Sentence column\n",
    "    df[col_name] = df[col_name].apply(lambda x: ' '.join([Word(x).lemmatize() for x in x.split()]))\n",
    "    # remove empty strings and strings with whitespace\n",
    "    df = df[df[col_name].str.strip().astype(bool)]\n",
    "    return df.dropna(ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0357881b-9fa5-44b7-9174-7fa39204f08b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>geosolutions technology leverage benefon gps s...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>esi low bk real possibility</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>last quarter componenta net sale doubled eurm ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>according finnishrussian chamber commerce majo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>swedish buyout firm sold remaining percent sta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Sentiment\n",
       "0  geosolutions technology leverage benefon gps s...         2\n",
       "1                        esi low bk real possibility         0\n",
       "2  last quarter componenta net sale doubled eurm ...         2\n",
       "3  according finnishrussian chamber commerce majo...         1\n",
       "4  swedish buyout firm sold remaining percent sta...         1"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_data = standardization(fin_data, \"Sentence\")\n",
    "fin_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01332fa1-c1e9-4680-86b0-b96bd810742a",
   "metadata": {},
   "source": [
    "### Splitting Data \n",
    "Create a dataset with predictor data (Sentence) and target data (Sentiment) and then split into training, validation, and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "f404955b-bbd2-4907-bba7-68ef06f146d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4088, 584, 1169)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tf.data.Dataset.from_tensor_slices((fin_data[\"Sentence\"].astype(str).values, fin_data[\"Sentiment\"].astype(int).values))\n",
    "data = data.shuffle(buffer_size = len(data))\n",
    "\n",
    "train_size = int(0.7*len(data))\n",
    "val_size   = int(0.1*len(data))\n",
    "\n",
    "train = data.take(train_size)\n",
    "val   = data.skip(train_size).take(val_size)\n",
    "test  = data.skip(train_size + val_size)\n",
    "\n",
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac439b-5238-4c6c-abc8-9ab289c1660c",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "We will now convert our text data into a vector where the frequency of the word defines the integer representation of the word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "7bfd7c08-b12a-4b43-86e9-f6d82261ef53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# only the top distinct words will be tracked\n",
    "max_tokens = 2500\n",
    "\n",
    "# each headline will be a vector of length 25\n",
    "# avg words in the Sentence column is 80\n",
    "sequence_length = 150\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens = max_tokens, # only consider this many words\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6250cc57-443a-46b6-b1c6-285247cbc3b2",
   "metadata": {},
   "source": [
    "In order for the vectorization to know the most frequent words, it needs to undergo an adaptation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "007d6ada-2c8e-4c11-b2d6-4dd902e25b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vectorize_sentence(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), [label]\n",
    "\n",
    "train_vec = train.map(vectorize_sentence)\n",
    "val_vec   = val.map(vectorize_sentence)\n",
    "test_vec  = test.map(vectorize_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40694b29-72cc-476a-a43e-39a45f23165b",
   "metadata": {},
   "source": [
    "Vectorize each data set using a `vectorize_sentence()` function that will accept two arguments (representing sentence and sentiment) and return two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "3d9d9e99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vectorize_sentence(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), [label]\n",
    "\n",
    "train_vec = train.map(vectorize_sentence)\n",
    "val_vec   = val.map(vectorize_sentence)\n",
    "test_vec  = test.map(vectorize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "9d0162b4-c16f-4885-a3c2-dd2c4ac23ffb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1 = tf.keras.Sequential([\n",
    "    layers.Embedding(max_tokens, output_dim =120, name=\"embedding\"),\n",
    "    layers.SpatialDropout1D(0.4),\n",
    "    layers.LSTM(704, dropout=0.2, recurrent_dropout=0.2),\n",
    "    layers.Dense(352, activation='LeakyReLU'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model2 = tf.keras.Sequential([\n",
    "  layers.Embedding(max_tokens, output_dim = 3, name=\"embedding\"),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "48c72636-3fe9-4593-8feb-3feb3004ef32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 120)         300000    \n",
      "                                                                 \n",
      " spatial_dropout1d_2 (Spati  (None, None, 120)         0         \n",
      " alDropout1D)                                                    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 704)               2323200   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 352)               248160    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 1059      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2872419 (10.96 MB)\n",
      "Trainable params: 2872419 (10.96 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "fe294038-c0aa-4290-a9e1-c0f7edbddc93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 3)           7500      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 3)           0         \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 3)                 0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 3)                 0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 3)                 12        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7512 (29.34 KB)\n",
      "Trainable params: 7512 (29.34 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "96f25ea8-1b0a-4023-baf2-978e203ff4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "# getting error\n",
    "# history = model1.fit(train_vec, epochs = 20, validation_data = val_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2d5333f9-ad11-4458-bd60-2b8a60ca7369",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/sarah/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/sarah/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/sarah/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/sarah/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1127, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/sarah/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/sarah/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/sarah/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/sarah/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/sarah/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Users/sarah/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/backend.py\", line 5575, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (1, 1) and (None, 3) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[247], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_vec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/hb/rdjmjqsd5xgffdyh7y5dcx4m0000gn/T/__autograph_generated_fileyonwajbg.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/sarah/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/sarah/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/sarah/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/sarah/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1127, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/sarah/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/sarah/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/sarah/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/sarah/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/sarah/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Users/sarah/anaconda3/envs/PIC16B-2/lib/python3.11/site-packages/keras/src/backend.py\", line 5575, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (1, 1) and (None, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(train_vec, epochs = 20, validation_data = val_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209bd424-56aa-4323-bebf-1b174ee043b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PIC16B-2] *",
   "language": "python",
   "name": "conda-env-PIC16B-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
